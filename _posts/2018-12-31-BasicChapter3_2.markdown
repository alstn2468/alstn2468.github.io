---
title:  "Tensorflow로 간단한 linear regression을 구현 (new)"
date:   2018-12-31 01:41:23
categories: [Machine Learnnig]
tags: [Machine Learnnig, Deep Learnnig, Data Science]
comments: true
---

해당 게시물은 [Edwith](https://www.edwith.org)에서 제공하는<br/>
[머신러닝과 딥러닝 BASIC](https://www.edwith.org/others26/joinLectures/9829)을 듣고 요약 정리한 글입니다.

<br/>

### Hypothesis and Cost function
**Hypothesis**
+ 주어진 `x`값에 대하여 어떻게 값을 **예측**할 것인가

\begin{align}
H(x) = Wx + b \\
\end{align}

**Cost function**
+ 값을 얼마나 잘 **예측**했는지 나타나는 것
+ `Cost function`을 **가장 작게** 만드는 것이 **학습**

\begin{align}
cost(W,b) = \frac{1}{m}\sum_{i=1}^m(H(x^{(i)}) - y^{(i)})^2 \\
\end{align}

<br/>

### 1. Build graph using TF operations

\begin{align}
H(x) = Wx + b \\
\end{align}



```python
import tensorflow as tf

# X and Y data
x_train = [1, 2, 3]
y_train = [1, 2, 3]

W = tf.Variable(tf.random_normal([1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

# Our hypothesis XW + b
hypothesis = x_train * W + b
```

    /anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.
      from ._conv import register_converters as _register_converters


\begin{align}
cost(W,b) = \frac{1}{m}\sum_{i=1}^m(H(x^{(i)}) - y^{(i)})^2 \\
\end{align}


```python
# Cost / Loss function
cost = tf.reduce_mean(tf.square(hypothesis - y_train))
```

**tf.reduce_mean()**
+ 주어진 값의 **평균**을 반환

**GradientDescent**


```python
# Minimize
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)
```

<br/>

### Run / Update graph and get results
`tensorflow`의 `variable`을 사용하기 전에는 **무조건**<br/>
`tf.global_variables_initializer()`함수를 사용해 **초기화**


```python
# Launch the graph in a session.
sess = tf.Session()
# Init global var in the graph.
sess.run(tf.global_variables_initializer())

# Fit the line
for step in range(2001):
    sess.run(train)

    if step % 20 == 0:
        print(step, sess.run(cost), sess.run(W), sess.run(b))
```

    0 0.34209618 [0.997927] [-0.58074147]
    20 0.033545386 [1.1831212] [-0.47202426]
    40 0.027953567 [1.19214] [-0.44208765]
    60 0.025365116 [1.1847885] [-0.420573]
    80 0.023036808 [1.1762639] [-0.40073743]
    100 0.020922398 [1.1679953] [-0.38189754]
    120 0.019002063 [1.1601015] [-0.36394912]
    140 0.017257968 [1.1525775] [-0.34684482]
    160 0.01567395 [1.1454071] [-0.33054435]
    180 0.014235348 [1.1385735] [-0.31501007]
    200 0.0129288 [1.1320611] [-0.30020583]
    220 0.011742125 [1.1258547] [-0.28609723]
    240 0.01066438 [1.1199398] [-0.27265167]
    260 0.009685551 [1.1143031] [-0.25983799]
    280 0.00879657 [1.1089313] [-0.24762647]
    300 0.0079891905 [1.103812] [-0.2359889]
    320 0.007255899 [1.098933] [-0.22489826]
    340 0.0065899272 [1.0942836] [-0.21432881]
    360 0.00598507 [1.0898526] [-0.20425609]
    380 0.0054357345 [1.0856298] [-0.19465671]
    400 0.0049368194 [1.0816054] [-0.18550855]
    420 0.0044836923 [1.0777702] [-0.17679021]
    440 0.004072161 [1.0741154] [-0.16848172]
    460 0.0036984005 [1.0706321] [-0.16056366]
    480 0.0033589455 [1.0673127] [-0.15301773]
    500 0.0030506484 [1.0641494] [-0.1458265]
    520 0.0027706444 [1.0611343] [-0.13897318]
    540 0.0025163395 [1.0582613] [-0.13244183]
    560 0.0022853736 [1.0555233] [-0.12621751]
    580 0.0020756186 [1.0529138] [-0.1202857]
    600 0.001885115 [1.0504271] [-0.1146327]
    620 0.0017120908 [1.0480573] [-0.10924539]
    640 0.0015549489 [1.0457988] [-0.10411128]
    660 0.0014122262 [1.0436463] [-0.09921843]
    680 0.0012826066 [1.0415952] [-0.09455557]
    700 0.0011648793 [1.0396402] [-0.0901117]
    720 0.00105796 [1.0377773] [-0.0858767]
    740 0.00096085883 [1.0360019] [-0.08184079]
    760 0.0008726657 [1.0343099] [-0.07799457]
    780 0.000792567 [1.0326974] [-0.07432906]
    800 0.00071982417 [1.0311608] [-0.07083588]
    820 0.00065375585 [1.0296963] [-0.06750689]
    840 0.0005937492 [1.0283008] [-0.06433428]
    860 0.0005392534 [1.0269707] [-0.06131082]
    880 0.0004897586 [1.0257032] [-0.05842944]
    900 0.00044481046 [1.0244952] [-0.05568347]
    920 0.00040398026 [1.023344] [-0.05306651]
    940 0.00036690236 [1.022247] [-0.05057259]
    960 0.00033322643 [1.0212014] [-0.04819589]
    980 0.0003026446 [1.0202051] [-0.04593093]
    1000 0.0002748634 [1.0192553] [-0.04377222]
    1020 0.00024963482 [1.0183504] [-0.041715]
    1040 0.00022672194 [1.017488] [-0.0397545]
    1060 0.00020591177 [1.016666] [-0.03788612]
    1080 0.00018700992 [1.0158828] [-0.03610559]
    1100 0.00016984683 [1.0151366] [-0.03440881]
    1120 0.00015425656 [1.0144252] [-0.03279176]
    1140 0.00014009811 [1.0137471] [-0.03125063]
    1160 0.00012724116 [1.0131011] [-0.02978192]
    1180 0.000115560404 [1.0124854] [-0.02838225]
    1200 0.0001049542 [1.0118985] [-0.02704835]
    1220 9.532145e-05 [1.0113394] [-0.02577715]
    1240 8.65722e-05 [1.0108066] [-0.02456572]
    1260 7.862677e-05 [1.0102987] [-0.0234113]
    1280 7.141052e-05 [1.0098147] [-0.02231106]
    1300 6.485634e-05 [1.0093534] [-0.02126252]
    1320 5.8903035e-05 [1.0089138] [-0.02026324]
    1340 5.3496104e-05 [1.0084949] [-0.01931092]
    1360 4.8587113e-05 [1.0080957] [-0.01840341]
    1380 4.412682e-05 [1.0077152] [-0.01753851]
    1400 4.007656e-05 [1.0073526] [-0.01671425]
    1420 3.6398487e-05 [1.0070071] [-0.01592876]
    1440 3.3058153e-05 [1.0066777] [-0.01518017]
    1460 3.0023424e-05 [1.0063639] [-0.01446675]
    1480 2.7268057e-05 [1.0060649] [-0.01378687]
    1500 2.4765388e-05 [1.0057799] [-0.01313897]
    1520 2.2492006e-05 [1.0055083] [-0.0125215]
    1540 2.042811e-05 [1.0052495] [-0.01193309]
    1560 1.8553206e-05 [1.0050027] [-0.01137231]
    1580 1.685074e-05 [1.0047677] [-0.0108379]
    1600 1.5303938e-05 [1.0045435] [-0.01032858]
    1620 1.3899029e-05 [1.00433] [-0.00984315]
    1640 1.2624049e-05 [1.0041267] [-0.00938063]
    1660 1.1465578e-05 [1.0039326] [-0.00893985]
    1680 1.0412673e-05 [1.0037478] [-0.00851968]
    1700 9.457023e-06 [1.0035717] [-0.00811931]
    1720 8.58898e-06 [1.0034039] [-0.00773777]
    1740 7.80083e-06 [1.0032438] [-0.00737414]
    1760 7.0849633e-06 [1.0030915] [-0.00702758]
    1780 6.4345245e-06 [1.0029461] [-0.00669735]
    1800 5.844049e-06 [1.0028077] [-0.00638261]
    1820 5.307595e-06 [1.0026759] [-0.00608269]
    1840 4.82041e-06 [1.00255] [-0.00579687]
    1860 4.3781924e-06 [1.0024303] [-0.00552446]
    1880 3.9767715e-06 [1.0023161] [-0.00526487]
    1900 3.611425e-06 [1.0022072] [-0.0050175]
    1920 3.2802097e-06 [1.0021034] [-0.00478174]
    1940 2.9790438e-06 [1.0020046] [-0.00455706]
    1960 2.7056e-06 [1.0019104] [-0.00434294]
    1980 2.4575631e-06 [1.0018208] [-0.00413892]
    2000 2.2321947e-06 [1.0017353] [-0.00394444]


<br/>

### Placeholders
사전에 `train data`를 설정하지 않고 `Session`을 실행시킬 때<br/>
`feed_dict`를 사용해 `train data` 지정이 가능하다.


```python
W = tf.Variable(tf.random_normal([1]), name='weight')
b = tf.Variable(tf.random_normal([1]), name='bias')

# Now we can use X and Y in place of x_data and y_data
# placeholders for a tensor tha will be always fed using feed_dict
X = tf.placeholder(tf.float32, shape=[None])
Y = tf.placeholder(tf.float32, shape=[None])

hypothesis = X * W + b
cost = tf.reduce_mean(tf.square(hypothesis - Y))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)
train = optimizer.minimize(cost)

sess = tf.Session()
sess.run(tf.global_variables_initializer())

# Fit the line
for step in range(2001):
    cost_val, W_val, b_val, _ = \
        sess.run([cost, W, b, train],
                feed_dict={X: [1, 2, 3, 4, 5],
                           Y: [2.1, 3.1, 4.1, 5.1, 6.1]})

    if step % 20 == 0:
        print(step, cost_val, W_val, b_val)
```

    0 11.289088 [0.35358387] [0.6417344]
    20 0.010992345 [1.0638802] [0.85676134]
    40 0.009398816 [1.0627136] [0.8735269]
    60 0.008208036 [1.0586201] [0.88836247]
    80 0.007168132 [1.0547811] [0.9022229]
    100 0.0062599713 [1.0511932] [0.91517556]
    120 0.005466898 [1.0478407] [0.92727983]
    140 0.004774262 [1.0447075] [0.9385914]
    160 0.004169411 [1.0417798] [0.94916224]
    180 0.0036411807 [1.0390434] [0.95904064]
    200 0.00317987 [1.0364865] [0.96827215]
    220 0.002777006 [1.034097] [0.97689915]
    240 0.0024251782 [1.0318639] [0.9849611]
    260 0.0021179225 [1.029777] [0.9924951]
    280 0.0018495988 [1.027827] [0.9995356]
    300 0.0016152605 [1.0260046] [1.0061152]
    320 0.0014106216 [1.0243014] [1.0122639]
    340 0.0012319067 [1.0227101] [1.0180097]
    360 0.0010758385 [1.0212228] [1.0233792]
    380 0.00093953824 [1.0198328] [1.0283973]
    400 0.0008204981 [1.018534] [1.0330868]
    420 0.00071654504 [1.0173199] [1.0374693]
    440 0.0006257567 [1.0161856] [1.0415646]
    460 0.00054647616 [1.0151256] [1.0453917]
    480 0.0004772433 [1.014135] [1.0489678]
    500 0.00041678114 [1.0132093] [1.0523099]
    520 0.0003639822 [1.0123442] [1.0554332]
    540 0.0003178666 [1.0115359] [1.0583516]
    560 0.0002776011 [1.0107805] [1.0610791]
    580 0.00024242947 [1.0100744] [1.0636281]
    600 0.00021171689 [1.0094146] [1.0660101]
    620 0.00018489076 [1.008798] [1.0682361]
    640 0.0001614657 [1.0082217] [1.0703166]
    660 0.00014100892 [1.0076833] [1.0722605]
    680 0.00012314346 [1.0071802] [1.0740772]
    700 0.00010754245 [1.0067099] [1.0757749]
    720 9.392006e-05 [1.0062705] [1.0773613]
    740 8.202072e-05 [1.0058599] [1.0788438]
    760 7.163043e-05 [1.0054762] [1.0802293]
    780 6.255442e-05 [1.0051175] [1.0815241]
    800 5.463013e-05 [1.0047823] [1.082734]
    820 4.7708098e-05 [1.0044692] [1.0838648]
    840 4.1665273e-05 [1.0041765] [1.0849214]
    860 3.6386897e-05 [1.003903] [1.0859088]
    880 3.1776268e-05 [1.0036474] [1.0868317]
    900 2.7750764e-05 [1.0034086] [1.0876939]
    920 2.42352e-05 [1.0031853] [1.0884999]
    940 2.1165635e-05 [1.0029767] [1.0892531]
    960 1.8480354e-05 [1.0027816] [1.0899576]
    980 1.6139435e-05 [1.0025995] [1.0906154]
    1000 1.4094636e-05 [1.0024291] [1.0912299]
    1020 1.2308749e-05 [1.00227] [1.0918043]
    1040 1.074942e-05 [1.0021214] [1.092341]
    1060 9.388066e-06 [1.0019825] [1.0928426]
    1080 8.198024e-06 [1.0018526] [1.0933114]
    1100 7.159786e-06 [1.0017313] [1.0937494]
    1120 6.253105e-06 [1.001618] [1.0941586]
    1140 5.4606608e-06 [1.001512] [1.0945411]
    1160 4.7687245e-06 [1.001413] [1.0948987]
    1180 4.1645444e-06 [1.0013204] [1.0952328]
    1200 3.6373112e-06 [1.001234] [1.0955449]
    1220 3.1763925e-06 [1.0011532] [1.0958365]
    1240 2.7744234e-06 [1.0010777] [1.096109]
    1260 2.4228366e-06 [1.0010071] [1.0963638]
    1280 2.1162505e-06 [1.0009413] [1.0966018]
    1300 1.8480727e-06 [1.0008798] [1.0968243]
    1320 1.6139018e-06 [1.0008221] [1.0970322]
    1340 1.4095725e-06 [1.0007683] [1.0972265]
    1360 1.2309991e-06 [1.0007179] [1.097408]
    1380 1.0752067e-06 [1.000671] [1.0975777]
    1400 9.3903253e-07 [1.000627] [1.0977364]
    1420 8.199281e-07 [1.0005859] [1.0978845]
    1440 7.1618706e-07 [1.0005475] [1.098023]
    1460 6.255058e-07 [1.0005118] [1.0981524]
    1480 5.462191e-07 [1.0004783] [1.0982732]
    1500 4.770899e-07 [1.0004469] [1.0983862]
    1520 4.1674247e-07 [1.0004178] [1.0984918]
    1540 3.6408133e-07 [1.0003905] [1.0985904]
    1560 3.1793948e-07 [1.0003649] [1.0986826]
    1580 2.777669e-07 [1.0003409] [1.0987687]
    1600 2.4259543e-07 [1.0003189] [1.0988493]
    1620 2.1188694e-07 [1.0002979] [1.0989246]
    1640 1.8513697e-07 [1.0002785] [1.098995]
    1660 1.6166857e-07 [1.0002602] [1.0990607]
    1680 1.4129925e-07 [1.0002433] [1.0991219]
    1700 1.2334017e-07 [1.0002272] [1.0991794]
    1720 1.0773766e-07 [1.0002126] [1.099233]
    1740 9.4123074e-08 [1.0001984] [1.0992832]
    1760 8.22517e-08 [1.0001857] [1.09933]
    1780 7.18329e-08 [1.0001736] [1.0993737]
    1800 6.275693e-08 [1.0001621] [1.0994147]
    1820 5.483298e-08 [1.0001516] [1.0994531]
    1840 4.7832373e-08 [1.0001417] [1.0994887]
    1860 4.1793328e-08 [1.0001323] [1.0995221]
    1880 3.6520042e-08 [1.0001237] [1.0995533]
    1900 3.1939056e-08 [1.0001158] [1.0995824]
    1920 2.794269e-08 [1.0001082] [1.0996096]
    1940 2.4392374e-08 [1.0001011] [1.099635]
    1960 2.1316282e-08 [1.0000944] [1.0996588]
    1980 1.8629601e-08 [1.0000883] [1.099681]
    2000 1.624063e-08 [1.0000825] [1.0997021]


<br/>

### Testing our model


```python
# Testing our model
print(sess.run(hypothesis, feed_dict={X: [5]}))
print(sess.run(hypothesis, feed_dict={X: [2.5]}))
print(sess.run(hypothesis, feed_dict={X: [1.5, 3.5]}))
```

    [6.100115]
    [3.5999084]
    [2.5998259 4.599991 ]
