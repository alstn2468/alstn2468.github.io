{"componentChunkName":"component---src-templates-blog-post-js","path":"/MachineLearning/2019-05-24-BasicChapter11-2","result":{"data":{"site":{"siteMetadata":{"title":"Minsu's Dev Log","author":"Minsu Kim","siteUrl":"https://alstn2468.github.io","comment":{"disqusShortName":"","utterances":"alstn2468/alstn2468.github.io"},"sponsor":{"buyMeACoffeeId":"eMLdEwU"}}},"markdownRemark":{"id":"bf0237eb-659d-5b82-878d-ab6528f8900b","excerpt":"해당 게시물은 Edwith에서 제공하는\n머신러닝과 딥러닝 BASIC을 듣고 요약 정리한 글입니다. Vanishing gradient를 해결하는 방법 ReLU 초기 Weight값 설정 랜덤으로 Weight값 설정시 같은 코드를 실행시켜도\nCost값이 변하는 것을 확인할 수 있다. 모든 Weight값을 0으로 초기화해서 사용할 경우 Chain rule을 사용할 때 Weight값이 사용된다.\nWeight값이 0이게 되면 뒤의 모든 값이 0이되어서\nVanishing gradient…","html":"<p>해당 게시물은 <a href=\"https://www.edwith.org\">Edwith</a>에서 제공하는<br/>\n<a href=\"https://www.edwith.org/others26/joinLectures/9829\">머신러닝과 딥러닝 BASIC</a>을 듣고 요약 정리한 글입니다.</p>\n<h2 id=\"vanishing-gradient를-해결하는-방법\" style=\"position:relative;\"><a href=\"#vanishing-gradient%EB%A5%BC-%ED%95%B4%EA%B2%B0%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95\" aria-label=\"vanishing gradient를 해결하는 방법 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Vanishing gradient를 해결하는 방법</h2>\n<ol>\n<li>ReLU</li>\n<li>초기 <strong>Weight</strong>값 설정</li>\n</ol>\n<p>랜덤으로 <strong>Weight</strong>값 설정시 같은 코드를 실행시켜도<br/>\n<strong>Cost</strong>값이 변하는 것을 확인할 수 있다.</p>\n<h2 id=\"모든-weight값을-0으로-초기화해서-사용할-경우\" style=\"position:relative;\"><a href=\"#%EB%AA%A8%EB%93%A0-weight%EA%B0%92%EC%9D%84-0%EC%9C%BC%EB%A1%9C-%EC%B4%88%EA%B8%B0%ED%99%94%ED%95%B4%EC%84%9C-%EC%82%AC%EC%9A%A9%ED%95%A0-%EA%B2%BD%EC%9A%B0\" aria-label=\"모든 weight값을 0으로 초기화해서 사용할 경우 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>모든 <strong>Weight</strong>값을 0으로 초기화해서 사용할 경우</h2>\n<p><strong>Chain rule</strong>을 사용할 때 <strong>Weight</strong>값이 사용된다.<br/>\n<strong>Weight</strong>값이 0이게 되면 뒤의 모든 값이 0이되어서<br/>\n<strong>Vanishing gradient</strong>가 생긴다.</p>\n<h2 id=\"어떻게-초기-weight값을-설정할-것인가\" style=\"position:relative;\"><a href=\"#%EC%96%B4%EB%96%BB%EA%B2%8C-%EC%B4%88%EA%B8%B0-weight%EA%B0%92%EC%9D%84-%EC%84%A4%EC%A0%95%ED%95%A0-%EA%B2%83%EC%9D%B8%EA%B0%80\" aria-label=\"어떻게 초기 weight값을 설정할 것인가 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>어떻게 초기 Weight값을 설정할 것인가?</h2>\n<ul>\n<li>무조건 <strong>0</strong>으로 설정하면 안된다.</li>\n<li>“A Fast Learning Algorithm for Deep Belief Nets”<br/>\n논문에서 <strong>R</strong>estricted <strong>B</strong>oatman <strong>M</strong>achine(<strong>RBM</strong>) 사용</li>\n</ul>\n<h2 id=\"restricted-boatman-machine-rbm\" style=\"position:relative;\"><a href=\"#restricted-boatman-machine-rbm\" aria-label=\"restricted boatman machine rbm permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Restricted Boatman Machine (RBM)</h2>\n<p>RBM의 구조<br/>\nReestriction : Layer안에서 연결이 없다.<br/>\n<span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 307px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 116.66666666666667%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAXCAYAAAALHW+jAAAACXBIWXMAAAsTAAALEwEAmpwYAAACqklEQVQ4y4WVWWsiURCF/f9v/gLJiw8BQfBNTFDE4BLjvu8aTeK+m01r+Gq4Pd2jJgWN3bdvnXvqnKrWdTqdhOD32nU8HuVS9Ho96Xa70m635fv7W9dcdrCfgvefn5+y2+1ktVpJOByWWq2m70ajkby9vf0FtAMdDgf5+vrS5P1+L9vtVp6fn2UwGEin05FWq6XJ6XRaPB6PAsIehsvl0gn48vIizWZTqtWqlMtlGQ6H0u/3deNisbDKBjgWi+n9er2Wer0u0+nUqsIq2dD/+PiQbDarpdmDZ5Kfnp6U+SVJHAwbjYbM53MtsVAoyGQy0TWYwpp7SmcPYRhTgblXhgYQ7QwD3JvNZrpO+TDmikajWh5VEGiHLEhFvsNlE4gOGGyCwaAlNoawbzweK4DX61XWRv/X19d/JRtAo00+n1emlAeLTCajBpCIBIlEQtmi+/v7ux5GKzk05ASS0YuNsDD95vf7dX2z2UilUlGWxmXA6IIzl0kgaF7aApY3NzeqJ4G+sDLum8m46rKhj4a5XE6f7+/v5fHxUXuPZ3SlC+yuQsAOagEaLQDEUTQkALy9vVWmvKOSYrGoTrPGQeSdzbIJ7GcD7XB3d6eOoxVmEOiFHG63W2UwLluzbAeEPpswA4MA4WsCG1oEKVgDgANhx8xD4GyWaRFKoVTKAoQDAAyFQioDzJggE7Dl4IuzbFzGxVKppAagkekvSsIcJge9f51lSoURPfbw8KAMcdPeHgBRZiqVsgaBQxx9aABpWpqZDZQAO7ShJO65aBn0SyaT4vP5FNzkYdzZ6F0LmJPIhfgcGAgELJeZMmuWf/vs/3QY1WCa42vDDXS5YIB2aEM7XPrzsjPGQBiTg9Zo7iKRDyptQdugUTwe19P/Z2l+AWFPJBLRfiSfLxCm/QFHbckNMCLc9gAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"아직 안만듬\" title=\"아직 안만듬\" src=\"/static/595d41b4de1f7ded42d8778c5a822ffe/4651d/2.png\" srcset=\"/static/595d41b4de1f7ded42d8778c5a822ffe/8a4e8/2.png 150w,\n/static/595d41b4de1f7ded42d8778c5a822ffe/5a46d/2.png 300w,\n/static/595d41b4de1f7ded42d8778c5a822ffe/4651d/2.png 307w\" sizes=\"(max-width: 307px) 100vw, 307px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span></p>\n<p>2가지 Operation을 갖는다.<br/></p>\n<ul>\n<li>Forward</li>\n</ul>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 349px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 95.33333333333334%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAATCAYAAACQjC21AAAACXBIWXMAAAsTAAALEwEAmpwYAAAC/0lEQVQ4y21U2XKbQBDU//+RK28up+yKHceS0YUU7kMcErcAcXRmFuFg2VMFLLA729PdszNco+97cU3HH+9867rh23V+d30mcQxN0xCGoXifTRdPk4yJ+qoC0nQYXxN3bQPUNSJNwWK5hmPbME2TpqWYjYim0dGisiz5B+Ad0P+8B1wHPY8tE7BsFK6KH68vmC+3sCgZozyfzwPCMUnbtrRxDVVVodIER3oHfEoSRejzDMhzAfPYpnh0bcQX4FKVMAxDoBMl5zRpv9/D9304joPtdtiRQ93tcLEMIMthZge85Qa8NsYmChHn7UgCpjoIhEVR4HK5iI9N0wj4uizDXcyJvxiIE+TaHu+bBwSOiedNiKWbi/nnckCYZdmA8HaHUcGauXIs4s5FFHvQ2wgVLbZUH7XpIJY1ZLqKv7RxRTTpUw6/U1ncDy7sQIejr2hMSeMWKXGd9gW86oi7pydIJArHJ5VvwyVUGgkj+zoSZU081JjLeyyl3wgqH2aRYHWIwSRFJBhrEATBUHJFPmNB2Jg2+WlHQui6Ln6+7VbYJRp+vTwSOuKoOSNZKzDdZDB2334VhW2iKIoglYWJyflsmzRJISkrLE4eJPJdoC2wjjIoAYlk0oanSCRhq52Oxw9Rvy05IS48MrJO9nh1DyjRQtED7N/nZJsEZhUiMjYkyg4GVcN246o4+RdR2OBjHJIWQX7GPU0+1GcyNpVNiwtSTMsL3D2/UOstxVym63Q6fY+wILP/2ajQggx+2ogO1DMfC0JsywZcxUVSD05wWECyjGVZA4eMiOtnLrl/WTXBIZW9klWE+QV2VMGNLjCTBNvcQ23sBdqxT1jYMWbcJZIkCavwxRbQyfkc8zWNwwJNNz2BOuIwwHFHXURAGgLEpY5JZ+PBcOtDVl53Q0LYEEJqr1MJ/VjCp1qzqod89PD2/EA2k4XluGyucjY9VLvrITra4f9x1guU/MzrlnitkBGHr4qB5WYt5lgkCltu9qXlJuPu5sD9HMP3wA9EM3BVHP8ArHqupuKLe7oAAAAASUVORK5CYII='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"아직 안만듬\" title=\"아직 안만듬\" src=\"/static/0240622ed618e6de24edf2b61cba3482/e9bf8/3.png\" srcset=\"/static/0240622ed618e6de24edf2b61cba3482/8a4e8/3.png 150w,\n/static/0240622ed618e6de24edf2b61cba3482/5a46d/3.png 300w,\n/static/0240622ed618e6de24edf2b61cba3482/e9bf8/3.png 349w\" sizes=\"(max-width: 349px) 100vw, 349px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span><br/>\n위의 그림과 같은 방법으로 값을 구하는 방법</p>\n<ul>\n<li>Backward</li>\n</ul>\n<p><span class=\"gatsby-resp-image-wrapper\" style=\"position: relative; display: block; margin-left: auto; margin-right: auto; max-width: 283px; \">\n      <span class=\"gatsby-resp-image-background-image\" style=\"padding-bottom: 117.33333333333333%; position: relative; bottom: 0; left: 0; background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAXCAYAAAALHW+jAAAACXBIWXMAAAsTAAALEwEAmpwYAAADl0lEQVQ4y4VVWW/bRhjU/39LXwsUKFokQNG3Aq2bC05tx0YFx4Zsiad4UyJFSrxFisdOdpeiZMcKQmAlapc7nJlvvtUIP7oI6Qe7uq6f2i9VTQNZliEIAjzP43Ojfg95MYb5A2DX9iiaCnJ/y2/1hwk2mw2/Z8BFURwBn5MiB7BnK1UJqDLI1Sfg41tor39Dud87n8+RpilGA9hisYCqqvB9v1e3l0aSBPCWIAzI0IFoA+I6wPkHpGGImShBFAUYhnGUzEDmFMw0TSiKglC36OSSypMBxwIJViDUL/5qdn87BvF7z7q2RV3XBxEcUNd17sXjdIpo5ePPn19B1JZYJzsU7RPJlg6ia4Ch7WvUvbCKAzIzRVHkoJKsUHIyvjj3sNIKdlhBpsO7nyFzKSvXQh3FKPdYbA+r8G636xkOHrIJx3HQUGnsiq1HyImGCpSiJMNfhDDMAMZEwaWR4Wy6geUsuFXMfxadlsofnaTOPuoKsSNCufuEPHD7BUWgzxIs4grnSoLf33057GH+x3F8ZMi+h0yRIc+3F2gcFRJSOI8P8I0A6mYHK9zi1sohzi0Y1FPfX3GGTN1okMs8tC0LgiRRdlS2JCL0I+g0yKpk4/zmAnqeoqNLopfTLulVrVYBXNdFWZbHKrMMDezWNBZ//foLZvYayxzI4gy7z+/52ixxcGkuQNpjV56sMnsDGzwZlOX4+h8o/hRmG2FlS4iEG8TRGnrQwcpCjCMRl8EUeduzavdtyQEHDxlLYTaDZdv8dx06SPUZYm0KgYK8vX4HM1ki70o8xAYeEwMlbUXWCMw/3/Ofx+ZpT5N9D5P7z4hpZdUNwa5I4dsT+HUGO/dxTVmeTS5QpDnfI1Hvt9vt6cOBCwiW2OgupsYSjav0WQ1drEMLTpNikhn4MLmi6dqdPhwimiGWpSTLeXDiuxmUVcmZLjcWFPkKamLTrhsj8FRekSoveTpY67JxqDILJKO8ppVWZAna+X+4mZtw2gRq5sCjMrN8jWqhoktChBd/ILdFDtDQgyHLsueHA6Nb5L0XRRzh79c/wQ1kxIFJ9beHI5qQDltPR0HBmjw6cbiTI0NGPWTn20zoQ1rmaCib7XLORxXYqOOABrvmgJkypvlvjlX2v6lyQg9SlsWBPvnmzU0eo04ClJ6GMnCQ3P2LydkbpEXFn2GEvlvlF/8xJ6Sh3kK4+x/V96p86g/q1NrT+azY8mKyphiq/BUdLtak2WyAYAAAAABJRU5ErkJggg=='); background-size: cover; display: block;\"></span>\n  <img class=\"gatsby-resp-image-image\" alt=\"아직 안만듬\" title=\"아직 안만듬\" src=\"/static/b5e523550d4252a0e23251d7379a06eb/bde6a/4.png\" srcset=\"/static/b5e523550d4252a0e23251d7379a06eb/8a4e8/4.png 150w,\n/static/b5e523550d4252a0e23251d7379a06eb/bde6a/4.png 283w\" sizes=\"(max-width: 283px) 100vw, 283px\" style=\"width:100%;height:100%;margin:0;vertical-align:middle;position:absolute;top:0;left:0;\" loading=\"lazy\">\n    </span><br/></p>\n<p>입력받은 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span>의 값과 <strong>Forward</strong>로 생성된 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span>의 값의<br/>\n<strong>차가 최저</strong>가 되도록 <strong>Weight</strong>를 조절하는 방법이다.</p>\n<p><strong>KL DIVERGENCE</strong>은 거리를 구할때 사용하는 연산자다.<br/>\n<strong>Encode</strong>/<strong>Decode</strong>라고도 한다.</p>\n<h2 id=\"어떻게-rbm이-작동하는가\" style=\"position:relative;\"><a href=\"#%EC%96%B4%EB%96%BB%EA%B2%8C-rbm%EC%9D%B4-%EC%9E%91%EB%8F%99%ED%95%98%EB%8A%94%EA%B0%80\" aria-label=\"어떻게 rbm이 작동하는가 permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>어떻게 RBM이 작동하는가?</h2>\n<p>앞의 2개의 Layer에서 <strong>Encode</strong>/<strong>Decode</strong>를<br/>\n반복하며 <strong>Weight</strong>값을 찾는다.<br/>\n이러한 Network를 <strong>D</strong>eep <strong>B</strong>elief <strong>N</strong>etwork (<strong>DBN</strong>)이라고 한다.</p>\n<p><strong>Pre Traning</strong>과정에서 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span>값만 가지고<br/>\n<strong>RBM</strong>을 진행해 최적화된 <strong>Weight</strong>를 찾는다.<br/>\n앞의 Layer에서 뒤의 Layer로 이동하면서 같은 과정을 반복한다.<br/>\n모든 과정이 완료 되면 모든 <strong>Weight</strong>값이 <strong>초기화된 상태</strong>다.</p>\n<p><strong>Fine Tuning</strong>과정에서 실제 <span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>X</mi></mrow><annotation encoding=\"application/x-tex\">X</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.07847em;\">X</span></span></span></span>데이터를 넣고 <strong>학습을 진행</strong>한다.<br/>\n<strong>Fine Tuning</strong>이라고 부르는 이유는 데이터를<br/>\n많이 쓰지않고 빠르게 학습이 되기 떄문이다.</p>\n<h2 id=\"good-news\" style=\"position:relative;\"><a href=\"#good-news\" aria-label=\"good news permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Good News</h2>\n<ul>\n<li>복잡한 <strong>RBM</strong>을 사용하지 않아도된다.</li>\n<li>\n<p>간단한 방법으로도 가능하다.</p>\n<ul>\n<li><strong>Xavier initialization</strong></li>\n<li><strong>He’s initialization</strong></li>\n</ul>\n</li>\n</ul>\n<h3 id=\"xavierhe-initialization\" style=\"position:relative;\"><a href=\"#xavierhe-initialization\" aria-label=\"xavierhe initialization permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>Xavier/He initialization</h3>\n<p><strong>좋은 Weight값을 선택</strong>하고자 하는 기본적인 아이디어<br/>\n입력의 갯수(<strong>fan_in</strong>), 출력의 갯수(<strong>fan_out</strong>)에 따라 구하면 된다.<br/></p>\n<p>Xavier initialization (2010)</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">W <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span>fan_in<span class=\"token punctuation\">,</span> fan_out<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> np<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>fan_in<span class=\"token punctuation\">)</span></code></pre></div>\n<p>He initialization (2015)</p>\n<div class=\"gatsby-highlight\" data-language=\"python\"><pre class=\"language-python\"><code class=\"language-python\">W <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span>fan_in<span class=\"token punctuation\">,</span> fan_out<span class=\"token punctuation\">)</span> <span class=\"token operator\">/</span> np<span class=\"token punctuation\">.</span>sqrt<span class=\"token punctuation\">(</span>fan_in <span class=\"token operator\">/</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span></code></pre></div>\n<p>복잡하게 구현되어 있는 <code class=\"language-text\">xavier_init</code>함수를 사용해도 된다.<br/></p>\n<p>초기 Weight을 설정하는 알고리즘은 아직도 연구중인 분야다.</p>","frontmatter":{"title":"Weight를 잘 초기화 해보자","date":"May 23, 2019","thumbnail":null}}},"pageContext":{"slug":"/MachineLearning/2019-05-24-BasicChapter11-2/","previous":{"fields":{"slug":"/MachineLearning/2019-05-22-BasicChapter10-5/"},"frontmatter":{"title":"Lab 9-2:Tensor Board로 딥네트웍 들여다보기","category":"Machine Learning","draft":false}},"next":{"fields":{"slug":"/MachineLearning/2019-05-23-BasicChapter11-1/"},"frontmatter":{"title":"XSigmoid 보다 ReLU가 더 좋아","category":"Machine Learning","draft":false}}}},"staticQueryHashes":["3001444076","3128451518"]}